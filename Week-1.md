# Hadoop Stack Basics

Apache Hadoop is Open Source software framework for storage and large-scale processing of data-sets. 
(Apache License)

Hadoop moves computation to data instead of the other way round. 

**Scalability** is at the core of it.<br/>
**Reliability** (hardware failures are handled automatically).

## The Apache Framework
Has 4 basic components:
1. **Hadoop Common**: Libraries and utilities needed by other Hadoop Components
2. **HDFS**: high bandwidth distributed storage
3. **Hadoop Yarn**: Manages computer resources and uses them to schedule users and applications
4. **Hadoop MapReduce**: Programming model that scales data across different processes
All the modules are designed with the scalability and reliability in mind. 

Other components: 
**Pig**: Scripting
**HBase**: Non relational database
**Oozie**: Workflow manager
**Hive**: SQL Query
**Sqoop**: Data Exchange