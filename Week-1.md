# Hadoop Stack Basics

Apache Hadoop is Open Source software framework for storage and large-scale processing of data-sets. 
(Apache License)

Hadoop moves computation to data instead of the other way round. 

**Scalability** is at the core of it.<br/>
**Reliability** (hardware failures are handled automatically).

## The Apache Framework

**4 basic components:**

1. **Hadoop Common**: Libraries and utilities needed by other Hadoop Components <br/>
2. **HDFS**: high bandwidth distributed storage <br/>
3. **Hadoop Yarn**: Manages computer resources and uses them to schedule users and applications <br/>
4. **Hadoop MapReduce**: Programming model that scales data across different processes <br/>
All the modules are designed with the scalability and reliability in mind.<br/>

**Other components: **

**Pig**: Scripting <br/>
**HBase**: Non relational database <br/>
**Oozie**: Workflow manager <br/>
**Hive**: SQL Query <br/>
**Sqoop**: Data Exchange <br/>